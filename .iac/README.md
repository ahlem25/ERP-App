# Infrastructure as Code - ERP Application

Ce r√©pertoire contient la configuration Terraform pour d√©ployer l'infrastructure AWS n√©cessaire √† l'application ERP.

## üèóÔ∏è Architecture

L'infrastructure comprend :
- **VPC** avec sous-r√©seaux publics et priv√©s
- **EKS Cluster unique** pour orchestrer les conteneurs
- **Kubernetes Namespaces** pour chaque environnement (dev, test, pprd, prod)
- **Kubernetes Dashboard** pour la gestion et le monitoring du cluster
- **ECR Repositories** pour stocker les images Docker (un par service et environnement)
- **RDS MySQL** : 4 instances s√©par√©es (une par environnement)
- **S3 Buckets** pour le stockage des fichiers upload√©s
- **Security Groups** pour la s√©curit√© r√©seau
- **IAM Roles** pour les permissions

### üéØ Avantages de l'architecture multi-namespace

- **Co√ªt r√©duit** : Un seul cluster EKS au lieu de plusieurs
- **Gestion simplifi√©e** : Un seul point de contr√¥le pour tous les environnements
- **Isolation** : Chaque environnement dans son propre namespace
- **Ressources partag√©es** : Les n≈ìuds sont partag√©s entre les environnements

## üìã Pr√©requis

- Terraform >= 1.0
- AWS CLI configur√©
- kubectl install√©
- Acc√®s AWS avec permissions appropri√©es

## üìÅ Structure des fichiers

- **`provider.tf`** : Configuration des providers Terraform (AWS, Kubernetes, Helm)
- **`variables.tf`** : D√©finition des variables d'entr√©e
- **`main.tf`** : Infrastructure principale (VPC, EKS, Security Groups, IAM)
- **`eks-access.tf`** : Configuration des acc√®s EKS et ConfigMap aws-auth
- **`eks_dashboard.tf`** : Configuration du dashboard Kubernetes avec Helm
- **`s3.tf`** : Configuration des buckets S3
- **`rds.tf`** : Configuration RDS MySQL (instances par environnement)
- **`ecr.tf`** : Configuration des repositories ECR (avec force_delete pour la destruction)
- **`policies.tf`** : Politiques IAM pour les services ERP
- **`outputs.tf`** : Variables de sortie

## üöÄ D√©ploiement

### 1. Configuration initiale

```bash
# Cloner le repository
git clone <repository-url>
cd .iac

# Optionnel : Personnaliser les variables par d√©faut
# Les valeurs par d√©faut sont d√©finies dans variables.tf
# Si vous voulez les modifier, cr√©ez un fichier terraform.tfvars :
cp terraform.tfvars.example terraform.tfvars
# Puis modifiez les valeurs selon vos besoins
vim terraform.tfvars
```

### 2. Initialisation Terraform

```bash
# 1. Initialiser Terraform
terraform init

# 2. Valider la configuration (recommand√©)
terraform validate

# 3. Formater
terraform fmt

# 4. Planifier: V√©rifier le plan de d√©ploiement
terraform plan

# 5. Appliquer: D√©ployer l'infrastructure
terraform apply -auto-approve

# NB: Probl√®eme dans eks-access.tf (END OF FILE)
dos2unix ./eks-access.tf 

# 6. Destruction: Destruction des Ressources
# Supprimer les nodes EKS en premier 
terraform destroy -target=aws_eks_cluster.main -target=aws_eks_node_group.main
# Supprimer les autres ressources 
terraform destroy -auto-approve

```

### 3. Validation de la configuration

#### **üîç Commandes de validation**

```bash
# Valider la syntaxe et la configuration
terraform validate

# Valider avec des variables sp√©cifiques
terraform validate -var="project_name=erp-app"

# Valider avec un fichier de variables
terraform validate -var-file="terraform.tfvars"

# Valider et formater le code
terraform fmt -check
```

#### **üìã V√©rifications automatiques**

```bash
# V√©rifier la syntaxe de tous les fichiers .tf
terraform validate

# Formater le code (corriger l'indentation)
terraform fmt

# V√©rifier le formatage sans le modifier
terraform fmt -check

# V√©rifier la configuration avec un plan
terraform plan -detailed-exitcode
```

#### **üö® R√©solution des erreurs de validation**

```bash
# Erreur de syntaxe
terraform validate
# ‚Üí Corriger les erreurs dans les fichiers .tf

# Erreur de formatage
terraform fmt -check
# ‚Üí Ex√©cuter: terraform fmt

# Erreur de variables manquantes
terraform validate
# ‚Üí Cr√©er terraform.tfvars ou d√©finir les variables

# Erreur de provider
terraform init -upgrade
# ‚Üí Mettre √† jour les providers
```

#### **‚úÖ Checklist de validation**

- [ ] **Syntaxe** : `terraform validate` sans erreurs
- [ ] **Formatage** : `terraform fmt -check` passe
- [ ] **Variables** : Toutes les variables requises d√©finies
- [ ] **Providers** : Versions compatibles install√©es
- [ ] **√âtat** : Pas de conflits dans l'√©tat Terraform
- [ ] **Plan** : `terraform plan` s'ex√©cute sans erreurs

### 4. Configuration kubectl

```bash
# Mettre √† jour la configuration kubectl
aws eks update-kubeconfig --region eu-west-3 --name erp-app-cluster

# V√©rifier la connexion
kubectl get nodes

# V√©rifier les namespaces cr√©√©s
kubectl get namespaces

# V√©rifier le dashboard Kubernetes (si activ√©)
kubectl get pods -n kubernetes-dashboard
```

## üìä Services d√©ploy√©s

### Kubernetes Namespaces
- `erp-dev` - Environnement de d√©veloppement
- `erp-test` - Environnement de test
- `erp-pprd` - Environnement de pr√©-production
- `erp-prod` - Environnement de production

### ECR Repositories (par type)
Pour chaque service, 2 repositories :
- **Stages** : Pour les images SNAPSHOT (d√©veloppement/test)
- **Releases** : Pour les images RELEASE (production)

#### Services backend :
- `erp-service-discovery-stages` / `erp-service-discovery-releases`
- `erp-config-stages` / `erp-config-releases`
- `erp-api-gateway-stages` / `erp-api-gateway-releases`
- `erp-user-service-stages` / `erp-user-service-releases`
- `erp-product-service-stages` / `erp-product-service-releases`
- `erp-inventory-service-stages` / `erp-inventory-service-releases`
- `erp-supplier-service-stages` / `erp-supplier-service-releases`
- `erp-order-service-stages` / `erp-order-service-releases`
- `erp-client-service-stages` / `erp-client-service-releases`
- `erp-payment-service-stages` / `erp-payment-service-releases`
- `erp-billing-service-stages` / `erp-billing-service-releases`
- `erp-sales-service-stages` / `erp-sales-service-releases`
- `erp-dashboard-service-stages` / `erp-dashboard-service-releases`
- `erp-scheduler-service-stages` / `erp-scheduler-service-releases`

#### Service UI :
- `erp-ui-service-stages` / `erp-ui-service-releases`

### RDS MySQL (Instances par environnement)
- **Instances** : 4 instances s√©par√©es (une par environnement)
- **Identifiants** : `erp-database-dev`, `erp-database-test`, `erp-database-pprd`, `erp-database-prod`
- **Engine** : MySQL 8.0.43
- **Instance Class** : `db.t3.micro`
- **Storage** : 20GB (max 100GB) par instance
- **Backup** : 7 jours de r√©tention
- **Bases de donn√©es** : `erp_dev`, `erp_test`, `erp_pprd`, `erp_prod`
- **Utilisateur** : `admin` / `erp_password_2024` (commun √† toutes les instances)

### S3 Bucket
- **Uploads Bucket** : Stockage des fichiers upload√©s par les utilisateurs
- **Chiffrement** : AES256
- **Versioning** : Activ√©
- **Lifecycle** : Transition automatique vers IA/Glacier pour optimiser les co√ªts

### Kubernetes Dashboard
- **Namespace** : `kubernetes-dashboard`
- **Installation** : Automatique via Helm chart
- **Version** : 7.0.0 (configurable)
- **Service** : ClusterIP par d√©faut (configurable)
- **RBAC** : R√¥les IAM et ClusterRoleBinding configur√©s
- **M√©triques** : ServiceMonitor activ√© pour Prometheus
- **S√©curit√©** : OIDC et IAM roles pour l'authentification

## üîß Configuration

### Variables principales

| Variable | Description | Valeur par d√©faut |
|----------|-------------|-------------------|
| `aws_region` | R√©gion AWS | `eu-west-3` |
| `environments` | Liste des environnements | `["dev", "test", "pprd", "prod"]` |
| `project_name` | Nom du projet | `erp-app` |
| `vpc_cidr` | CIDR du VPC | `10.0.0.0/16` |
| `node_desired_size` | Nombre de n≈ìuds | `2` |

### Configuration RDS pour l'application

Apr√®s le d√©ploiement, r√©cup√©rez les informations RDS n√©cessaires pour configurer votre backend :

```bash
# R√©cup√©rer la configuration RDS compl√®te par environnement
terraform output environment_databases

# R√©cup√©rer des informations sp√©cifiques
terraform output rds_endpoint
terraform output rds_port
```

#### Configuration dans votre application Spring Boot

Ajoutez ces variables d'environnement dans votre configuration :

```yaml
# application.yml
spring:
  datasource:
    url: jdbc:mysql://erp-database-${ENVIRONMENT}.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306/erp_${ENVIRONMENT}?useSSL=false
    username: admin
    password: erp_password_2024
    driver-class-name: com.mysql.cj.jdbc.Driver
```

#### URLs de connexion par environnement

- **Dev** : `jdbc:mysql://erp-database-dev.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306/erp_dev?useSSL=false`
- **Test** : `jdbc:mysql://erp-database-test.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306/erp_test?useSSL=false`
- **PPRD** : `jdbc:mysql://erp-database-pprd.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306/erp_pprd?useSSL=false`
- **Prod** : `jdbc:mysql://erp-database-prod.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306/erp_prod?useSSL=false`

#### Endpoints RDS par environnement

- **Dev** : `erp-database-dev.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306`
- **Test** : `erp-database-test.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306`
- **PPRD** : `erp-database-pprd.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306`
- **Prod** : `erp-database-prod.cnsyguwokgsk.eu-west-3.rds.amazonaws.com:3306`

### Configuration S3 pour l'application

Apr√®s le d√©ploiement, r√©cup√©rez les informations S3 n√©cessaires pour configurer votre backend :

```bash
# R√©cup√©rer la configuration S3 compl√®te
terraform output s3_configuration

# R√©cup√©rer des informations sp√©cifiques
terraform output s3_uploads_bucket_name
terraform output s3_uploads_bucket_arn
```

#### Configuration dans votre application Spring Boot

Ajoutez ces variables d'environnement dans votre configuration :

```yaml
# application.yml
aws:
  s3:
    region: eu-west-3
    uploads-bucket: erp-app-uploads-xxxxx
```

#### Permissions IAM

Les n≈ìuds EKS ont automatiquement les permissions pour :
- `s3:GetObject` - Lire les fichiers
- `s3:PutObject` - √âcrire les fichiers
- `s3:DeleteObject` - Supprimer les fichiers
- `s3:ListBucket` - Lister le contenu des buckets

#### Utilisation dans le code

```java
// Exemple de configuration Spring Boot
@Value("${aws.s3.uploads-bucket}")
private String uploadsBucket;

@Value("${aws.s3.region}")
private String region;

// Configuration du client S3
@Bean
public S3Client s3Client() {
    return S3Client.builder()
        .region(Region.of(region))
        .build();
}
```

### Configuration du Dashboard Kubernetes

Le dashboard Kubernetes est automatiquement install√© et configur√© via Terraform. Voici comment l'utiliser :

#### **üöÄ D√©marrage rapide**

```bash
# 1. V√©rifier que le dashboard est d√©ploy√©
kubectl get pods -n kubernetes-dashboard

# 2. R√©cup√©rer le token d'authentification
kubectl -n kubernetes-dashboard get secret dashboard-admin-token -o jsonpath='{.data.token}' | base64 -d

# 3. Acc√©der au dashboard via Kong proxy (recommand√©)
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard-kong-proxy 8443:443
# Ouvrir https://localhost:8443 dans le navigateur et utiliser le token
```

#### **üîß Variables de configuration**

| Variable | Description | D√©faut |
|----------|-------------|---------|
| `dashboard_enabled` | Activer le dashboard | `true` |
| `dashboard_namespace` | Namespace pour le dashboard | `kubernetes-dashboard` |
| `dashboard_chart_version` | Version du chart Helm | `7.0.0` |
| `dashboard_service_type` | Type de service | `ClusterIP` |
| `dashboard_service_port` | Port du service | `443` |
| `dashboard_ingress_enabled` | Activer l'ingress | `false` |
| `dashboard_metrics_enabled` | Activer les m√©triques | `true` |
| `create_dashboard_rbac` | Cr√©er les r√¥les RBAC | `true` |
| `create_load_balancer` | Cr√©er un Load Balancer | `false` |

#### **üåê Acc√®s au Dashboard**

##### **1. Port Forward via Kong Proxy (recommand√©)**
```bash
# Mettre √† jour la configuration kubectl
aws eks update-kubeconfig --region eu-west-3 --name erp-app-cluster

# V√©rifier les namespaces cr√©√©s
kubectl get namespaces

# Port Forward via Kong proxy (recommand√© pour √©viter les probl√®mes CSRF)
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard-kong-proxy 8443:443
```
Acc√©dez √† : `https://localhost:8443`

##### **2. Port Forward direct (alternative)**
```bash
# Port Forward direct vers le service web
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard-web 8080:8000
```
Acc√©dez √† : `http://localhost:8080`

Pour r√©cup√©rer le token **il faut utiliser la m√©thode 4 pour que tout fonctionne correctement ‚úÖ (les autes m√©thodes j'arrive pas √† les tester)**

##### **3. Via Ingress (production)**
Si vous avez configur√© un ingress, acc√©dez √† l'URL configur√©e :
`https://dashboard.yourdomain.com`

##### **4. Via Load Balancer**
Si vous avez activ√© le Load Balancer, utilisez l'URL fournie dans les outputs.

#### **üîê Authentification**

##### **R√©cup√©rer le token d'authentification**

**üîç Diagnostic pr√©alable :**
```bash
# 1. V√©rifier que le namespace existe
kubectl get namespace kubernetes-dashboard

# 2. V√©rifier que le service account existe
kubectl get serviceaccount -n kubernetes-dashboard

# 3. V√©rifier les secrets du service account
kubectl get secrets -n kubernetes-dashboard | grep kubernetes-dashboard

# 4. V√©rifier si le service account a des secrets associ√©s
kubectl -n kubernetes-dashboard get sa kubernetes-dashboard -o jsonpath='{.secrets[0].name}'
# Si cette commande retourne vide, le service account n'a pas de secret (normal avec les versions r√©centes)

# 5. V√©rifier la version de kubectl
kubectl version --client
# Si la version est < 1.24, la M√©thode 1 ne fonctionnera pas
```

**M√©thode 1 : Commande moderne (‚ö†Ô∏è N√©cessite kubectl 1.24+)**
```bash
# Cette m√©thode fonctionne UNIQUEMENT avec kubectl 1.24+ et Kubernetes 1.24+
# Votre version actuelle : kubectl 1.21.2 (trop ancienne)
kubectl -n kubernetes-dashboard create token kubernetes-dashboard

# Si vous obtenez "unknown command 'token'", utilisez la M√©thode 4 (industrialis√©e)
```

**M√©thode 2 : R√©cup√©ration depuis le secret (compatible)**
```bash
# √âtape 1 : Trouver le nom du secret
SECRET_NAME=$(kubectl -n kubernetes-dashboard get sa kubernetes-dashboard -o jsonpath='{.secrets[0].name}')

# √âtape 2 : R√©cup√©rer le token
kubectl -n kubernetes-dashboard get secret $SECRET_NAME -o jsonpath='{.data.token}' | base64 -d
```

**M√©thode 3 : Alternative simple (‚ö†Ô∏è Ne fonctionne pas avec le service account par d√©faut)**
```bash
# Cette m√©thode ne fonctionne pas car le service account kubernetes-dashboard n'a pas de secret
# Utilisez plut√¥t la M√©thode 4 (industrialis√©e) ou la M√©thode 5 (manuelle)
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa kubernetes-dashboard -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 -d
```

**M√©thode 4 : Solution industrialis√©e avec Terraform (recommand√©e)**
```bash
# 1. R√©cup√©rer les informations du token
terraform output dashboard_admin_token

# 2. Ex√©cuter la commande fournie pour r√©cup√©rer le token
kubectl -n kubernetes-dashboard get secret dashboard-admin-token -o jsonpath='{.data.token}' | base64 -d

# 3. Acc√©der au dashboard
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8443:443
# Puis aller sur https://localhost:8443 et utiliser le token
```

**M√©thode 5 : Solution manuelle (si Terraform n'est pas utilis√©)**
```bash
# 1. Cr√©er un service account temporaire avec permissions admin
kubectl create serviceaccount temp-admin -n kubernetes-dashboard
kubectl create clusterrolebinding temp-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:temp-admin

# 2. Cr√©er un secret pour le service account
kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: temp-admin-token
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: temp-admin
type: kubernetes.io/service-account-token
EOF

# 3. R√©cup√©rer le token
kubectl -n kubernetes-dashboard get secret temp-admin-token -o jsonpath='{.data.token}' | base64 -d
```

**üìã R√©sum√© des m√©thodes :**
- **M√©thode 4** (industrialis√©e) : **Recommand√©e** - Automatique et fiable via Terraform
- **M√©thode 1** : ‚ö†Ô∏è **N√©cessite kubectl 1.24+** - Votre version (1.21.2) est trop ancienne
- **M√©thode 2** : Compatible - Fonctionne avec toutes les versions de kubectl
- **M√©thode 3** : ‚ö†Ô∏è **Ne fonctionne pas** - Le service account par d√©faut n'a pas de secret
- **M√©thode 5** : Manuelle - Pour les cas o√π Terraform n'est pas utilis√©

**üö® D√©pannage :**
- Si le namespace n'existe pas : `kubectl create namespace kubernetes-dashboard`
- Si le service account n'existe pas : Le dashboard n'est pas encore d√©ploy√©
- Si les secrets sont vides : Probl√®me de configuration RBAC
- **Pourquoi la M√©thode 1 ne fonctionne pas** : Vous avez kubectl 1.21.2, mais la commande `create token` n√©cessite kubectl 1.24+. Utilisez la M√©thode 4 (industrialis√©e) ou la M√©thode 5 (manuelle).
- **Pourquoi la M√©thode 3 ne fonctionne pas** : Le service account `kubernetes-dashboard` cr√©√© par Helm n'a pas de secret associ√© (c'est normal avec les versions r√©centes de Kubernetes). Utilisez la M√©thode 4 (industrialis√©e) ou la M√©thode 5 (manuelle) qui cr√©ent un service account avec secret.
- **Mise √† jour de kubectl** (optionnel) : `brew upgrade kubectl` (macOS) ou t√©l√©charger depuis [kubernetes.io](https://kubernetes.io/docs/tasks/tools/)

##### **üîß R√©solution du probl√®me d'erreur CSRF**

Si vous rencontrez l'erreur `Unknown error (200): Http failure during parsing for http://localhost:8000/api/v1/csrftoken/login` :

**üéØ Solution recommand√©e :**
```bash
# 1. Utiliser le port-forward via Kong proxy (√©vite les probl√®mes CSRF)
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard-kong-proxy 8443:443

# 2. Acc√©der via le navigateur sur https://localhost:8443
# 3. S√©lectionner "Token" comme m√©thode d'authentification
# 4. Coller le token r√©cup√©r√© avec la commande ci-dessous
kubectl -n kubernetes-dashboard get secret dashboard-admin-token -o jsonpath='{.data.token}' | base64 -d
```

**üîç Pourquoi cette erreur se produit :**
- Le dashboard utilise Kong comme proxy pour g√©rer les requ√™tes
- L'acc√®s direct au service web peut causer des probl√®mes de validation CSRF
- Le port-forward via Kong proxy r√©sout ces probl√®mes

**‚úÖ V√©rification que tout fonctionne :**
```bash
# 1. V√©rifier que le dashboard est accessible
curl -k -I https://localhost:8443

# 2. Tester l'API CSRF via Kong
curl -k -X GET https://localhost:8443/api/v1/csrftoken/login

# 3. V√©rifier que le token est valide
kubectl -n kubernetes-dashboard get secret dashboard-admin-token -o jsonpath='{.data.token}' | base64 -d | wc -c
```

##### **Cr√©er un utilisateur admin (optionnel)**

**1. Cr√©er l'utilisateur admin**
```bash
# Cr√©er un service account admin
kubectl create serviceaccount admin-user -n kubernetes-dashboard

# Lui donner les permissions d'administrateur
kubectl create clusterrolebinding admin-user --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:admin-user
```

**2. R√©cup√©rer le token de l'utilisateur admin**

**M√©thode 1 : Commande moderne (‚ö†Ô∏è N√©cessite kubectl 1.24+)**
```bash
# Cette m√©thode fonctionne UNIQUEMENT avec kubectl 1.24+ et Kubernetes 1.24+
# Votre version actuelle : kubectl 1.21.2 (trop ancienne)
kubectl -n kubernetes-dashboard create token admin-user

# Si vous obtenez "unknown command 'token'", utilisez la M√©thode 4 (industrialis√©e)
```

**M√©thode 2 : R√©cup√©ration depuis le secret**
```bash
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa admin-user -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 -d
```

**M√©thode 3 : Alternative avec jq (si install√©)**
```bash
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa admin-user -o jsonpath='{.secrets[0].name}') -o json | jq -r '.data.token' | base64 -d
```

#### **üìä Monitoring et M√©triques**

Les m√©triques sont activ√©es par d√©faut. Pour les visualiser :
```bash
# V√©rifier les m√©triques
kubectl get servicemonitor -n kubernetes-dashboard

# Port forward pour Prometheus (si configur√©)
kubectl port-forward -n monitoring service/prometheus-server 9090:80
```

#### **üîß Configuration avanc√©e**

##### **Activer l'ingress**
```hcl
# Dans terraform.tfvars
dashboard_ingress_enabled = true
dashboard_ingress_class   = "nginx"
dashboard_ingress_hosts = [
  {
    host  = "dashboard.yourdomain.com"
    paths = ["/"]
  }
]
```

##### **Activer le Load Balancer**
```hcl
# Dans terraform.tfvars
create_load_balancer = true
load_balancer_annotations = {
  "service.beta.kubernetes.io/aws-load-balancer-type" = "nlb"
}
```

#### **üìù Outputs utiles**

```bash
# Afficher tous les outputs du dashboard
terraform output | grep dashboard

# Afficher les URLs d'acc√®s
terraform output dashboard_access_urls

# Afficher les commandes kubectl
terraform output dashboard_kubectl_commands

# Afficher la configuration
terraform output dashboard_configuration

# R√©cup√©rer les informations du token d'authentification
terraform output dashboard_admin_token
```

#### **üè≠ Industrialisation avec Terraform**

Le dashboard Kubernetes est enti√®rement industrialis√© avec Terraform :

**‚úÖ Ressources cr√©√©es automatiquement :**
- Service Account `dashboard-admin` avec permissions cluster-admin
- ClusterRoleBinding pour les permissions
- Secret `dashboard-admin-token` pour l'authentification
- Configuration RBAC compl√®te

**‚úÖ Avantages de l'approche Terraform :**
- **Reproductible** : M√™me configuration √† chaque d√©ploiement
- **Versionnable** : Configuration dans le code source
- **Automatis√©e** : Pas d'intervention manuelle
- **Idempotente** : Peut √™tre ex√©cut√©e plusieurs fois sans probl√®me
- **Int√©gr√©e** : Fait partie de l'infrastructure compl√®te

**‚úÖ Utilisation compl√®te (workflow recommand√©) :**
```bash
# 1. V√©rifier que le dashboard est d√©ploy√©
kubectl get pods -n kubernetes-dashboard

# 2. R√©cup√©rer les informations du token
terraform output dashboard_admin_token

# 3. R√©cup√©rer le token d'authentification
kubectl -n kubernetes-dashboard get secret dashboard-admin-token -o jsonpath='{.data.token}' | base64 -d

# 4. Acc√©der au dashboard via Kong proxy (√©vite les probl√®mes CSRF)
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard-kong-proxy 8443:443

# 5. Ouvrir le navigateur sur https://localhost:8443
# 6. S√©lectionner "Token" et coller le token r√©cup√©r√© √† l'√©tape 3
```

**‚úÖ Commandes de v√©rification :**
```bash
# V√©rifier les ressources cr√©√©es
kubectl get serviceaccount -n kubernetes-dashboard | grep dashboard-admin
kubectl get clusterrolebinding | grep dashboard-admin
kubectl get secret -n kubernetes-dashboard | grep dashboard-admin-token

# V√©rifier les permissions
kubectl auth can-i '*' '*' --as=system:serviceaccount:kubernetes-dashboard:dashboard-admin
```

**‚úÖ Test complet du dashboard :**
```bash
# 1. V√©rifier que tout fonctionne
kubectl get pods -n kubernetes-dashboard
kubectl get serviceaccount -n kubernetes-dashboard | grep dashboard-admin
kubectl get secret -n kubernetes-dashboard | grep dashboard-admin-token

# 2. R√©cup√©rer et tester le token
TOKEN=$(kubectl -n kubernetes-dashboard get secret dashboard-admin-token -o jsonpath='{.data.token}' | base64 -d)
echo "Token r√©cup√©r√©: ${TOKEN:0:50}..."

# 3. Tester l'acc√®s au dashboard via Kong proxy (recommand√©)
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard-kong-proxy 8443:443 &
echo "Dashboard accessible sur: https://localhost:8443"
echo "Token: $TOKEN"

# 4. V√©rifier que le dashboard r√©pond correctement
sleep 5
curl -k -I https://localhost:8443
curl -k -X GET https://localhost:8443/api/v1/csrftoken/login
```

#### **üö® D√©pannage**

##### **Probl√®mes courants**
1. **Dashboard inaccessible** : V√©rifiez les pods avec `kubectl get pods -n kubernetes-dashboard`
2. **Erreur d'authentification** : V√©rifiez les r√¥les RBAC avec `kubectl get clusterrolebinding kubernetes-dashboard`
3. **Probl√®mes d'ingress** : V√©rifiez que le contr√¥leur d'ingress est install√©

##### **Commandes de diagnostic**
```bash
# V√©rifier l'√©tat des pods
kubectl get pods -n kubernetes-dashboard

# V√©rifier les services
kubectl get services -n kubernetes-dashboard

# V√©rifier les logs
kubectl logs -n kubernetes-dashboard deployment/kubernetes-dashboard

# V√©rifier les √©v√©nements
kubectl get events -n kubernetes-dashboard
```

### Tags

Toutes les ressources sont tagu√©es avec :
- `Project` : erp-app
- `ManagedBy` : terraform

## üîí S√©curit√©

### Security Groups
- **EKS Cluster** : Communication interne uniquement
- **EKS Nodes** : Communication avec le cluster et internet
- **RDS** : Acc√®s depuis les n≈ìuds EKS uniquement

### IAM Roles
- **EKS Cluster** : Permissions pour g√©rer le cluster
- **EKS Nodes** : Permissions pour les n≈ìuds worker

## üîê Gestion des Acc√®s EKS (eks-access.tf)

Le fichier `eks-access.tf` g√®re automatiquement l'acc√®s au cluster EKS via le ConfigMap aws-auth.

### üéØ Configuration des Acc√®s

#### **1. ConfigMap aws-auth**
- **R√¥les EKS** : Configuration automatique des r√¥les des n≈ìuds
- **Utilisateurs** : Jenkins et Root User avec acc√®s complet
- **Groupes** : `system:masters` pour les administrateurs

#### **2. Utilisateurs Configur√©s**
- **jenkins-user** : `arn:aws:iam::ACCOUNT_ID:user/jenkins-user`
- **root** : `arn:aws:iam::ACCOUNT_ID:root`
- **Groupes** : `system:masters` (acc√®s complet au cluster)

#### **3. V√©rification Automatique**
- Test d'acc√®s au cluster apr√®s configuration
- Affichage des informations du cluster
- Validation des permissions

### üìã Commandes Utiles

```bash
# Mettre √† jour kubeconfig
aws eks update-kubeconfig --region eu-west-3 --name erp-app-cluster

# V√©rifier l'acc√®s
kubectl get nodes
kubectl get namespaces

# Voir la configuration aws-auth
kubectl get configmap aws-auth -n kube-system -o yaml
```

## üîê Gestion des Politiques IAM (policies.tf)

Le fichier `policies.tf` g√®re les permissions IAM pour les services ERP.

### üéØ R√¥le et Utilit√©

#### **1. Acc√®s EKS pour Jenkins**
- **Probl√®me** : M√™me avec `AdministratorAccess`, Jenkins ne peut pas d√©ployer dans EKS par d√©faut
- **Solution** : Ajout automatique de l'utilisateur `jenkins-user` au configmap `aws-auth`
- **Permissions** : `system:masters` (acc√®s complet au cluster)

#### **2. Configuration Automatique**
```hcl
# Ajout automatique de Jenkins au cluster EKS
resource "null_resource" "add_jenkins_to_aws_auth" {
  # Attendre que le cluster soit pr√™t
  # Mettre √† jour kubeconfig
  # Ajouter l'utilisateur au configmap aws-auth
}
```

#### **3. V√©rification d'Acc√®s**
- **Test automatique** : V√©rification que Jenkins peut acc√©der au cluster
- **Validation** : Commande `kubectl get nodes` pour confirmer l'acc√®s
- **Logs** : Messages de confirmation ou d'erreur

### üìã Ressources Cr√©√©es

#### **Pour Jenkins :**
- **Data source** : R√©f√©rence √† l'utilisateur `jenkins-user` existant
- **Configmap update** : Ajout automatique au `aws-auth`
- **Verification** : Test d'acc√®s au cluster EKS

#### **Pour les Services ERP :**
- **`erp_services_rds_access`** : Acc√®s aux m√©tadonn√©es RDS
- **`erp_services_secrets_access`** : Acc√®s aux secrets AWS Secrets Manager

### üîß Configuration Requise

#### **Pr√©requis :**
1. **Utilisateur IAM** : `jenkins-user` doit exister
2. **Permissions** : `AdministratorAccess` ou permissions EKS/ECR
3. **Cluster EKS** : Doit √™tre cr√©√© avant l'ex√©cution des politiques

#### **D√©ploiement :**
```bash
# Appliquer les politiques
terraform apply -target=null_resource.add_jenkins_to_aws_auth

# V√©rifier l'acc√®s
terraform apply -target=null_resource.verify_jenkins_eks_access
```

### üö® Points Importants

#### **S√©curit√© :**
- **Principe du moindre privil√®ge** : Seules les permissions n√©cessaires sont accord√©es
- **Isolation** : Chaque service a ses propres politiques
- **Audit** : Toutes les actions sont trac√©es dans CloudTrail

#### **Maintenance :**
- **Automatique** : La configuration se fait automatiquement lors du d√©ploiement
- **Idempotent** : Peut √™tre ex√©cut√© plusieurs fois sans probl√®me
- **D√©tection** : V√©rifie si l'utilisateur existe d√©j√† avant de l'ajouter

### üìä Output Disponible

```hcl
output "jenkins_eks_access" {
  value = {
    user_arn  = "arn:aws:iam::ACCOUNT_ID:user/jenkins-user"
    username  = "jenkins-user"
    groups    = ["system:masters"]
    cluster   = "erp-app-cluster"
    region    = "eu-west-3"
  }
}
```

### üîç V√©rification Manuelle

```bash
# V√©rifier que Jenkins est dans aws-auth
kubectl get configmap aws-auth -n kube-system -o yaml

# Tester l'acc√®s (avec les credentials de jenkins-user)
aws sts get-caller-identity
kubectl get nodes
kubectl get pods
```

## üìà Monitoring

### CloudWatch
- Logs du cluster EKS
- M√©triques des n≈ìuds
- Logs RDS

### Kubernetes Dashboard
- Interface web pour la gestion du cluster
- Visualisation des pods, services, namespaces
- M√©triques et logs en temps r√©el
- Gestion des ressources et d√©ploiements

### Co√ªts estim√©s
- **EKS Cluster** : ~$73/mois
- **EC2 Nodes (4x t3.large)** : ~$240/mois
- **RDS MySQL (4x db.t3.micro)** : ~$60/mois (4 instances)
- **S3 Bucket** : ~$5-15/mois (selon le volume de donn√©es)
- **Total estim√©** : ~$378-388/mois

## üóëÔ∏è Destruction des Ressources

### **‚ö†Ô∏è ATTENTION : Destruction compl√®te**

La destruction supprimera **TOUTES** les ressources AWS cr√©√©es par Terraform, y compris :
- Cluster EKS et tous les pods/services
- Base de donn√©es RDS (avec perte de donn√©es)
- Repositories ECR et toutes les images
- VPC, sous-r√©seaux, NAT Gateway
- Security Groups et IAM policies

### **üöÄ Commandes de destruction**

#### **1. Destruction standard**
```bash
# Aller dans le r√©pertoire Terraform
cd .iac

# Voir ce qui sera d√©truit (recommand√©)
terraform plan -destroy

# D√©truire l'infrastructure
terraform destroy

# Confirmer la destruction
yes
```

#### **2. Destruction forc√©e (si n√©cessaire)**
```bash
# Destruction sans confirmation interactive
terraform destroy -auto-approve

# Destruction avec variables sp√©cifiques
terraform destroy -var="project_name=erp-app" -auto-approve
```

#### **3. Destruction s√©lective**
```bash
# D√©truire seulement certaines ressources
terraform destroy -target=aws_eks_cluster.main
terraform destroy -target=aws_db_instance.mysql
terraform destroy -target=aws_ecr_repository.erp_services

# D√©truire le dashboard Kubernetes
terraform destroy -target=helm_release.kubernetes_dashboard
terraform destroy -target=kubernetes_namespace.kubernetes_dashboard

# D√©truire par type de ressource
terraform destroy -target='aws_ecr_repository.*'
terraform destroy -target='aws_security_group.*'
```

### **üîç V√©rification apr√®s destruction**

```bash
# V√©rifier que les ressources sont supprim√©es
aws eks list-clusters --region eu-west-3
aws rds describe-db-instances --region eu-west-3
aws ecr describe-repositories --region eu-west-3

# V√©rifier l'√©tat Terraform
terraform show
terraform state list
```

### **üö® Probl√®mes courants et solutions**

#### **Erreur : "Resource is in use"**
```bash
# Forcer la destruction (attention aux d√©pendances)
terraform destroy -auto-approve

# Ou d√©truire dans l'ordre inverse
terraform destroy -target=aws_eks_node_group.main
terraform destroy -target=aws_eks_cluster.main
```

#### **Erreur : "Cannot delete VPC"**
```bash
# V√©rifier les ressources restantes
aws ec2 describe-vpcs --vpc-ids vpc-xxxxxxxxx
aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=vpc-xxxxxxxxx"

# Supprimer manuellement si n√©cessaire
aws ec2 delete-internet-gateway --internet-gateway-id igw-xxxxxxxxx
aws ec2 delete-vpc --vpc-id vpc-xxxxxxxxx
```

#### **Erreur : "RDS deletion protection"**
```bash
# D√©sactiver la protection avant destruction
aws rds modify-db-instance \
    --db-instance-identifier erp-app-mysql \
    --no-deletion-protection \
    --apply-immediately
```

### **üíæ Sauvegarde avant destruction**

```bash
# Sauvegarder l'√©tat Terraform
cp terraform.tfstate terraform.tfstate.backup

# Exporter les outputs importants
terraform output -json > outputs.json

# Sauvegarder la base de donn√©es RDS
aws rds create-db-snapshot \
    --db-instance-identifier erp-app-mysql \
    --db-snapshot-identifier erp-app-mysql-backup-$(date +%Y%m%d)
```

### **üîÑ Nettoyage complet**

```bash
# Supprimer les fichiers Terraform temporaires
rm -rf .terraform/
rm -f .terraform.lock.hcl
rm -f terraform.tfstate*

# Supprimer les fichiers de configuration kubectl
rm -f ~/.kube/config

# Nettoyer les caches AWS
aws configure list-profiles
# Supprimer les profils inutiles si n√©cessaire
```

## üìù Notes importantes

1. **Co√ªts** : Cette infrastructure g√©n√®re des co√ªts AWS
2. **S√©curit√©** : Changez les mots de passe par d√©faut
3. **Backup** : Configurez des sauvegardes automatiques
4. **Monitoring** : Activez CloudWatch pour le monitoring
5. **Scaling** : Ajustez les param√®tres selon vos besoins
6. **Dashboard** : Le dashboard Kubernetes est activ√© par d√©faut, d√©sactivez-le si non n√©cessaire
7. **Acc√®s** : Utilisez le port-forward pour acc√©der au dashboard en local

## üîó Liens utiles

- [Documentation EKS](https://docs.aws.amazon.com/eks/)
- [Documentation ECR](https://docs.aws.amazon.com/ecr/)
- [Documentation RDS](https://docs.aws.amazon.com/rds/)
- [Documentation Terraform AWS](https://registry.terraform.io/providers/hashicorp/aws/latest)
